{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Reordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "\n",
    "1. First extract the models downloaded from [here](https://drive.google.com/file/d/1UX5_6E7vOiBzFoO0tCh5Pu5CzAWm2flE/view?usp=sharing) into the `tree_text_gen` base directory. \n",
    "\n",
    "2. Download the `personachat` sentences dataset here: [train](https://drive.google.com/file/d/1JnBTNKiVJGLB3tcyYo36NTNvvWXIsX8P/view?usp=sharing)\n",
    " [valid](https://drive.google.com/file/d/179s3ONLEqMEaGjdueuEC4XQyXhh7Sck0/view?usp=sharing) [test](https://drive.google.com/file/d/12VhPsvp-RgQzg9TowYKO6Ym2Wscd4o1A/view?usp=sharing). Put these files in the `data_dir` specified in the next step.\n",
    " \n",
    "2. Set `data_dir` to the directory holding the dataset files from (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/sw1986/datasets/personachat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import tree_text_gen.binary.bagorder.evaluate as evaluate\n",
    "from pprint import pprint as pp\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will load the correct experiment directories if you did the setup steps correctly. The cell prints the directory names and the contents of an example experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/sw1986/projects/phd/tree_text_gen/models/bagorder/leftright',\n",
      " '/home/sw1986/projects/phd/tree_text_gen/models/bagorder/uniform',\n",
      " '/home/sw1986/projects/phd/tree_text_gen/models/bagorder/annealed']\n",
      "leftright  leftright.checkpoint  model_config.json  tok2i.json\n"
     ]
    }
   ],
   "source": [
    "import tree_text_gen\n",
    "project_dir = os.path.abspath(os.path.join(os.path.dirname(tree_text_gen.__file__), os.pardir))\n",
    "dirs = glob(os.path.join(project_dir, 'models/bagorder/*'))\n",
    "pp(dirs)\n",
    "d = dirs[0]\n",
    "!ls $d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load each model from `dirs`\n",
    "\n",
    "`exprs` will map experiment/model name to pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leftright\n",
      "uniform\n",
      "annealed\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = False\n",
    "\n",
    "exprs = {}\n",
    "for d in dirs:\n",
    "    expr_name = d.split('/')[-1]\n",
    "    exprs[expr_name] = d\n",
    "\n",
    "models = {}\n",
    "for k, v in exprs.items():\n",
    "    print(k)\n",
    "    models[k] = evaluate.load_model(v, k, checkpoint=CHECKPOINT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16181 sentences\n"
     ]
    }
   ],
   "source": [
    "kind = 'valid'\n",
    "# kind = 'test'\n",
    "\n",
    "from tree_text_gen.binary.common.data import load_personachat, build_tok2i, SentenceDataset, inds2toks\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tok2i = list(models.values())[0].tok2i\n",
    "dataset = load_personachat(os.path.join(data_dir, 'personachat_all_sentences_%s.jsonl' % kind))\n",
    "dataset = SentenceDataset(dataset, tok2i)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=dataset.collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/505 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n",
      "=== leftright === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 505/505 [01:35<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('em', 0.2301) ('f1', 0.91)\n",
      "BLEU+case.mixed+numrefs.1+smooth.none+tok.13a+version.1.2.12 = 46.6 89.5/53.7/36.7/27.1 (BP = 0.997 ratio = 0.997 hyp_len = 195217 ref_len = 195797)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/505 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== uniform === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 505/505 [01:36<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('em', 0.2095) ('f1', 0.9676)\n",
      "BLEU+case.mixed+numrefs.1+smooth.none+tok.13a+version.1.2.12 = 44.7 96.6/53.7/33.7/23.4 (BP = 0.995 ratio = 0.995 hyp_len = 194822 ref_len = 195789)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/505 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== annealed === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 505/505 [01:36<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('em', 0.23) ('f1', 0.9613)\n",
      "BLEU+case.mixed+numrefs.1+smooth.none+tok.13a+version.1.2.12 = 46.8 96.1/55.6/36.3/25.9 (BP = 0.989 ratio = 0.989 hyp_len = 193595 ref_len = 195768)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pp\n",
    "print(kind)\n",
    "for name, model in models.items():\n",
    "    print(\"=== %s === \" % (name))\n",
    "    ms, predictions = evaluate.eval_dataset(model, dataloader)\n",
    "    print(('em', ms['eval/em']), ('f1', ms['eval/f1']))\n",
    "    !cat __hyp.txt | sacrebleu --force --smooth none __ref.txt\n",
    "\n",
    "!rm __hyp.txt\n",
    "!rm __ref.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "import seaborn as sns\n",
    "\n",
    "def viz_step_distributions(model, scores, preds, x, out):\n",
    "    ps = F.softmax(scores[0], dim=1)\n",
    "    content_timesteps = ((preds[0] != model.tok2i['<end>']) & (preds[0] != model.tok2i['<p>'])).nonzero().view(-1)[:len(out['genorder_tokens'])]\n",
    "    input_word_idxs = x[0][x[0] != model.tok2i['</s>']]\n",
    "\n",
    "    label_ps = []\n",
    "    for t in content_timesteps:\n",
    "        label_ps.append(ps[t][input_word_idxs])\n",
    "\n",
    "    label_ps = th.stack(label_ps).cpu().numpy()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    sns.heatmap(label_ps, ax=ax, cmap='gist_gray', cbar=True, vmin=0., vmax=1.)\n",
    "    ax.set_xticklabels(out['gt_tokens'], rotation='vertical')\n",
    "    ax.set_yticklabels(list(enumerate(out['genorder_tokens'])), rotation='horizontal')\n",
    "    ax.set_xlabel('ground-truth tokens')\n",
    "    ax.set_ylabel('(time, generated word)')\n",
    "    ax.set_title('Per-step probabilities over ground-truth tokens')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select and Display an example from the validation set\n",
    "\n",
    "Each row of the heatmap shows a policy's token probabilities at a given step, with only probabilities for correct actions shown. The y-axis shows which token the policy sampled at that step.\n",
    "\n",
    "The annealed policy tends to display a lower entropy distribution than the uniform policy. Intuitively, this shows the annealed policy's learned preferences over the set of valid actions at each step, while the uniform policy tries to put uniform mass on all valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16181 sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8178ea27e549abbaf909f502e3a4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=8090, description='idx', max=16180), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from tree_text_gen.binary.common.data import load_personachat, build_tok2i, SentenceDataset, inds2toks\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "valid = load_personachat(os.path.join(data_dir, 'personachat_all_sentences_valid.jsonl'))\n",
    "sentences = list(sorted([' '.join(x['tokens']) for x in valid], key=lambda x: len(x)))\n",
    "\n",
    "def run(idx):\n",
    "    sentence = sentences[idx]\n",
    "    for name, model in models.items():\n",
    "        out, x, scores, preds = evaluate.eval_single(model, sentence)\n",
    "\n",
    "        print('=== %s ===' % name)\n",
    "        evaluate.print_output(out)\n",
    "        viz_step_distributions(model, scores, preds, x, out)\n",
    "        print('---------------------------------------------------')\n",
    "\n",
    "interact(run, idx=(0, len(sentences)-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
